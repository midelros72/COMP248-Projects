# LLM Configuration
# -----------------
# Set to 'true' to use local Ollama model
USE_OLLAMA=true
OLLAMA_MODEL=mistral:latest
OLLAMA_BASE_URL=http://localhost:11434

# OpenAI Configuration (Optional if using Ollama)
# OPENAI_API_KEY=sk-...
# OPENAI_MODEL=gpt-4o-mini

# Mistral Configuration (Optional)
# MISTRAL_API_KEY=...
# MISTRAL_MODEL=mistral-small-latest
